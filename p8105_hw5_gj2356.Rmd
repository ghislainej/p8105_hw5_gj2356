---
title: "Homework 5"
output: github_document
date: "2023-11-15"
author: "Ghislaine Jumonville"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

```{r, include=FALSE}
library(tidyverse)
library(rvest)
library(readr)
```

## PROBLEM 1

The code chunk below reads in the Washington Post homicide data, and creates a new variable `city_state` that combines the city and state information.
```{r, message = FALSE}
urlfile = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

homicide = read_csv(url(urlfile)) |>
          janitor::clean_names() |>
          mutate(
            city_state = paste(city, state, sep = ", "),
            status = case_match(disposition,
                                c("Closed without arrest","Open/No arrest") ~ "unsolved",
                                .default = "solved")
          )
```

There are 52179 observations and 13 variables in the homicide dataset detailing homicides in 50 states over a 10 year period (January 2007 to December 2017). Variables include date of homicide (`reported_date`), name of victim (`victim_last` and `victim_first`), victim demographics (`victim_race`, `victim_sex`, and `victim_age`), where the crime occurred (`city`, `state`, `city_state`, `lat`, `long`), as well as the status of the case - whether the case is open or closed and if there has been an arrest (`disposition`).

```{r, message = FALSE}
#number of homicides by city
homicides_city = homicide |>
                  group_by(city_state) |> 
                  summarize(n = n()) |> 
                  knitr::kable(caption = "Number of Homicides by City")
homicides_city

#number of unsolved homicides by city
unsolved_homicides_city = homicide |> 
                            filter(status == "unsolved") |> 
                            group_by(city_state) |> 
                            summarize(n = n()) |> 
                            knitr::kable(caption = "Number of Unsolved Homicides by City")
unsolved_homicides_city
```

## PROBLEM 2

The code chunk below creates a data frame containing all the names of the files. Each of the file names include the subject ID and arm (experimental or control).
```{r}
all_file_names = tibble(
                    filename = list.files("./data")
                    ) |> 
                mutate(
                  path = paste("./data/", sep = "", filename)
                ) |> 
                select(path)
```

The code chunk below create a function to read a file and then the function `load_hw5_files` is iterated over all the files in the folder to create one large data frame, `all_files`. This data frame contains the subject ID, what experimental arm they were in, and the observations for each of the weeks 1 - 8.
```{r, message = FALSE}
load_hw5_files = function(path, range) {
  df = 
    read_csv(path) |> 
    janitor::clean_names()
  df
}

all_files = all_file_names |> 
              mutate(
                week_observation = map(all_file_names$path, load_hw5_files),
                arm_ID = gsub("./data/","",path),
                arm_ID = gsub(".csv","",arm_ID)) |>
              separate(arm_ID, into = c("arm", "ID")) |> 
              mutate(
                arm = case_match(arm,
                      "con" ~ "control",
                      "exp" ~ "experimental"),
                ID = sub("^0","", ID)) |> 
              unnest(cols = "week_observation") |> 
              select(ID, arm, week_1:week_8)
knitr::kable(all_files)
```


The code chunk below creates a plot showing the two groups: control and experimental, and how each subject observation value changes over time.
```{r}
all_files_plot = all_files |> 
                    pivot_longer(
                      cols = starts_with("week_"),
                      names_to = "week",
                      values_to = "observation"
                      ) |> 
                    mutate(
                      week = as.numeric(gsub("week_","", week)),
                      week = factor(week)
                      ) |> 
                    ggplot(aes(x = week, y = observation, group = ID, color = ID)) +
                    geom_line() +
                    geom_point() +
                    facet_grid(~arm) +
                    labs(title = "Observations Over Time for Control and Experimental Groups")
                    
all_files_plot
```

Looking at the plots, one can see that the control group have lower observation values compared to the experimental group. The experimental group seems to have an upward trend. The control group has fluctuations throughout the weeks, but end up around the same values that they started at.


## PROBLEM 3

```{r, include = FALSE}
set.seed(123)
```


The code chunk below creates a function that will generate samples from a normal distribution in which only the true mean of the distribution will change with the input while the sample size (n = 30) and standard deviation will remain the same (sigma = 5).
```{r}
sim_t_test = function(mu) {
  sim_data = 
    tibble(
        x = rnorm(30, mean = mu, sd = 5))
  
  t.test(sim_data$x, alternative = "two.sided", conf.level = 0.95) |>
    broom::tidy() |>
    select(estimate, p.value)
}
```


The code chunk below creates a data frame containing 30,000 observations.There are 5,000 samples that were generated using the function with their respective t-test results (mean estimate and p-value). This was done for values of mu 0:5, which has resulted in 30,000 observations in this final dataset.
```{r}
mu_results_df = 
  expand_grid(
    mu = c(0, 1, 2, 3, 4, 5),
    iter = 1:5000) |> 
  mutate(
    output = map(mu, sim_t_test)
  ) |> 
  unnest(output)

head(mu_results_df)
```

The code chunk below creates a plot that compares the proportion of time we rejected the null hypothesis, meaning that our p-value was greater than 0.05 and the true mean. Looking at the plot below we can see that as effect size increases (true mean) the proportion of time we reject the null hypothesis also increases. This is because that as the effect size increases our power (the likelihood we reject the null hypothesis given that the alternative is true) also increases.
```{r}
null_reject_plot = 
  mu_results_df |> 
  filter(p.value < 0.05) |> 
  group_by(mu) |> 
  summarize(
    total_n = n()) |> 
  mutate(
    prop_reject_null = total_n/5000
  ) |> 
  ggplot(aes(x = mu, y = prop_reject_null)) + 
    geom_line() + 
    labs(
      title = "Effect Size and Power",
      x = "True Mean",
      y = "Proportion of Times the Null was Rejected")

null_reject_plot
```


The code chunk below creates a plot that is looking at the average mean estimates for all the samples and then for only the samples in which the null was rejected for each value of the true mean. The sample average mean estimate across tests for which the null was rejected is further away from the true mean than the sample average mean estimate across all tests. However, as the true mean increases the value of the average mean estimate when the null is rejected gets closer to the actual value. This is because as the effect size gets larger the average mean estimates will get closer to the true mean. 
```{r}
avg_mu_hat_plot = 
  mu_results_df |> 
  group_by(mu) |> 
  summarize(
    avg_mu_hat = mean(estimate),
    avg_mu_hat_rejected = mean(estimate[p.value < 0.05])
  ) |> 
  pivot_longer(
    cols = starts_with("avg"),
    names_to = "mu_hat",
    values_to = "avg_mean_estimate"
  ) |> 
  ggplot(aes(x = mu, y = avg_mean_estimate, color = mu_hat)) + 
  geom_point() +
  labs(
    title = "Average Mean Estimate vs. True Mean",
    x = "True Mean",
    y = "Average Mean Estimate"
  ) +
  scale_color_manual(
    values = c("turquoise", "pink"),
    labels = c("All Samples", "Null Rejected"),
    name = "Average Mean Estimate"
  )

avg_mu_hat_plot
```